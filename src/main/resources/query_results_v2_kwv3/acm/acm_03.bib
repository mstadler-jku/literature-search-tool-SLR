@inproceedings{10.1145/2740908.2743045,
author = {Fertig, Tobias and Braun, Peter},
title = {Model-Driven Testing of RESTful APIs},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2743045},
doi = {10.1145/2740908.2743045},
abstract = {In contrast to the increasing popularity of REpresentational State Transfer (REST), systematic testing of RESTful Application Programming Interfaces (API) has not attracted much attention so far. This paper describes different aspects of automated testing of RESTful APIs. Later, we focus on functional and security tests, for which we apply a technique called model-based software development. Based on an abstract model of the RESTful API that comprises resources, states and transitions a software generator not only creates the source code of the RESTful API but also creates a large number of test cases that can be immediately used to test the implementation. This paper describes the process of developing a software generator for test cases using state-of-the-art tools and provides an example to show the feasibility of our approach.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1497–1502},
numpages = {6},
keywords = {measurement, languages, verification},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@article{10.1007/s00165-013-0286-3,
author = {Frappier, Marc and Gervais, Fr\'{e}d\'{e}ric and Laleau, R\'{e}gine and Milhau, J\'{e}r\'{e}my},
title = {Refinement Patterns for ASTDs},
year = {2014},
issue_date = {Sep 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {5},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-013-0286-3},
doi = {10.1007/s00165-013-0286-3},
abstract = {This paper introduces three refinement patterns for algebraic state-transition diagrams (astds): state refinement, transition refinement and loop-transition refinement. These refinement patterns are derived from practice in using astds for specifying information systems and security policies in two industrial research projects. Two refinement relations used in these patterns are formally defined. For each pattern, proof obligations are proposed to ensure preservation of behaviour through refinement. The proposed refinement relations essentially consist in preserving scenarios by replacing abstract events with concrete events, or by introducing new events. Deadlocks cannot be introduced; divergence over new events is allowed in one of the refinement relation. We prove congruence-like properties for these three patterns, in order to show that they can be applied to a subpart of a specification while preserving global properties. These three refinement patterns are illustrated with a simple case study of a complaint management system.},
journal = {Form. Asp. Comput.},
month = {sep},
pages = {919–941},
numpages = {23},
keywords = {astd, Refinement, Patterns, Information systems}
}

@inproceedings{10.1145/3052973.3053003,
author = {Kim, Jongkil and Susilo, Willy and Guo, Fuchun and Au, Man Ho and Nepal, Surya},
title = {An Efficient KP-ABE with Short Ciphertexts in Prime OrderGroups under Standard Assumption},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3053003},
doi = {10.1145/3052973.3053003},
abstract = {We introduce an efficient Key-Policy Attribute-Based Encryption (KP-ABE) scheme in prime order groups. Our scheme is semi-adaptively secure under the decisional linear assumption and supports a large universe of attributes and multi-use of attributes. Those properties are critical for real applications of KP-ABE schemes since they enable an efficient and flexible access control. Prior to our work, existing KP-ABE schemes with short ciphertexts were in composite order groups or utilized either Dual Pairing Vector Spaces (DPVS) or Dual System Groups (DSG) in prime order groups. However, those techniques brought an efficiency loss. In this work, we utilize a nested dual system encryption which is a variant of Waters' dual system encryption (Crypto' 09) to achieve semi-adaptively secure KP-ABE. As a result, we obtain a new scheme having better efficiency compared to existing schemes while it keeps a semi-adaptive security under the standard assumption. We implement our scheme and compare its efficiency with the previous best work.},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {823–834},
numpages = {12},
keywords = {prime order groups, dual system encryption, standard assumption, short ciphertexts, attribute based encryption},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@inproceedings{10.1145/3445969.3450426,
author = {Shakarami, Mehrnoosh and Sandhu, Ravi},
title = {Role-Based Administration of Role-Based Smart Home IoT},
year = {2021},
isbn = {9781450383196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445969.3450426},
doi = {10.1145/3445969.3450426},
abstract = {Using role-based access control (RBAC) to manage RBAC is among RBAC's attractive benefits, contributing to its long-standing dominance in practice. Administrative models facilitate management of (mostly configuration) changes in the underlying operational models. Overall system security is crucially dependent on both the administrative and operational models. In this paper, we develop an RBAC administrative model to manage authorization assignments in the EGRBAC (enhanced generalized role-based access control) operational model for smart home IoT. We design the administrative model based on pairwise disjoint Administrative Units, each of which contains a uniquely assigned administrative role and a set of administrative tasks. Administrative tasks determine the administrative permissions available to manage the operational model assignments. We begin with a model containing a single administrative unit and then extend it to include additional units. Multiple administrative units enable decentralized administration which could be adapted to provide scalability in inherently distributed and large-scale environments beyond smart home, such as smart buildings or smart campuses. We provide formalism of our proposed model and illustrate it by specifying operational and administrative use cases. Although, the model is proposed based on a specific smart home operational model, our approach could be applied to environments with similar dynamics.},
booktitle = {Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {49–58},
numpages = {10},
keywords = {decentralized administration, smart home, RBAC administrative model},
location = {Virtual Event, USA},
series = {SAT-CPS '21}
}

@article{10.1145/514183.514185,
author = {Fielding, Roy T. and Taylor, Richard N.},
title = {Principled Design of the Modern Web Architecture},
year = {2002},
issue_date = {May 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/514183.514185},
doi = {10.1145/514183.514185},
abstract = {The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia application. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this article we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture and used to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support.},
journal = {ACM Trans. Internet Technol.},
month = {may},
pages = {115–150},
numpages = {36},
keywords = {World Wide Web, REST, Network-based applications}
}

@inproceedings{10.1145/2396276.2396281,
author = {Lim, Jong Hyun and Zhan, Andong and Goldschmidt, Evan and Ko, JeongGil and Chang, Marcus and Terzis, Andreas},
title = {HealthOS: A Platform for Pervasive Health Applications},
year = {2012},
isbn = {9781450317641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396276.2396281},
doi = {10.1145/2396276.2396281},
abstract = {Pervasive health applications that compose longitudinal information streams to infer people's health and encourage lifestyle changes have the potential to substantially benefit public health. Off-the-shelf medical and wellness sensors meet the sensing requirements of such applications but their closed and vertically-integrated designs impede composability and complicate unified management. Furthermore, the lack of security and privacy controls discourages individuals from sharing their data. This paper presents HealthOS, a development and execution framework for pervasive health applications. HealthOS addresses the sensor and system incompatibility challenge through a set of adapters. Moreover, pipeline modules translate custom formats and protocols to the requirements of target applications/systems. These modules execute in HealthOS servers, programmable devices that expose Representational State Transfer (REST) interfaces for data retrieval and sensor management. HealthOS servers can store data locally or push them to untrusted, third party services. Finally, HealthOS leverages attribute-based encryption to offer sophisticated role-based and content-based access controls for users' data.},
booktitle = {Proceedings of the Second ACM Workshop on Mobile Systems, Applications, and Services for HealthCare},
articleno = {4},
numpages = {6},
keywords = {RESTful API, pervasive health applications, platform},
location = {Toronto, Ontario, Canada},
series = {mHealthSys '12}
}

@inproceedings{10.1145/2897937.2897992,
author = {Nahiyan, Adib and Xiao, Kan and Yang, Kun and Jin, Yeir and Forte, Domenic and Tehranipoor, Mark},
title = {AVFSM: A Framework for Identifying and Mitigating Vulnerabilities in FSMs},
year = {2016},
isbn = {9781450342360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897937.2897992},
doi = {10.1145/2897937.2897992},
abstract = {A finite state machine (FSM) is responsible for controlling the overall functionality of most digital systems and, therefore, the security of the whole system can be compromised if there are vulnerabilities in the FSM. These vulnerabilities can be created by improper designs or by the synthesis tool which introduces additional don't-care states and transitions during the optimization and synthesis process. An attacker can utilize these vulnerabilities to perform fault injection attacks or insert malicious hardware modifications (Trojan) to gain unauthorized access to some specific states. To our knowledge, no systematic approaches have been proposed to analyze these vulnerabilities in FSM. In this paper, we develop a framework named Analyzing Vulnerabilities in FSM (AVFSM) which extracts the state transition graph (including the don't-care states and transitions) from a gate-level netlist using a novel Automatic Test Pattern Generation (ATPG) based approach and quantifies the vulnerabilities of the design to fault injection and hardware Trojan insertion. We demonstrate the applicability of the AVFSM framework by analyzing the vulnerabilities in the FSM of AES and RSA encryption module. We also propose a low-cost mitigation technique to make FSM more secure against these attacks.},
booktitle = {Proceedings of the 53rd Annual Design Automation Conference},
articleno = {89},
numpages = {6},
location = {Austin, Texas},
series = {DAC '16}
}

@inproceedings{10.1145/2897845.2897858,
author = {Zhang, Kai and Gong, Junqing and Tang, Shaohua and Chen, Jie and Li, Xiangxue and Qian, Haifeng and Cao, Zhenfu},
title = {Practical and Efficient Attribute-Based Encryption with Constant-Size Ciphertexts in Outsourced Verifiable Computation},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897858},
doi = {10.1145/2897845.2897858},
abstract = {In cloud computing, computationally weak users are always willing to outsource costly computations to a cloud, and at the same time they need to check the correctness of the result provided by the cloud. Such activities motivate the occurrence of verifiable computation (VC). Recently, Parno, Raykova and Vaikuntanathan showed any VC protocol can be constructed from an attribute-based encryption (ABE) scheme for a same class of functions. In this paper, we propose two practical and efficient semi-adaptively secure key-policy attribute-based encryption (KP-ABE) schemes with constant-size ciphertexts. The semi-adaptive security requires that the adversary designates the challenge attribute set after it receives public parameters but before it issues any secret key query, which is stronger than selective security guarantee. Our first construction deals with small universe while the second one supports large universe. Both constructions employ the technique underlying the prime-order instantiation of nested dual system groups, which are based on the $d$-linear assumption including SXDH and DLIN assumptions. In order to evaluate the performance, we implement our ABE schemes using $textsf{Python}$ language in Charm. Compared with previous KP-ABE schemes with constant-size ciphertexts, our constructions achieve shorter ciphertext and secret key sizes, and require low computation costs, especially under the SXDH assumption.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {269–279},
numpages = {11},
keywords = {charm, dual system encryption, verifiable computation, outsourced computation, attribute-based encryption},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@inproceedings{10.1145/3545948.3545980,
author = {Franzen, Fabian and Holl, Tobias and Andreas, Manuel and Kirsch, Julian and Grossklags, Jens},
title = {Katana: Robust, Automated, Binary-Only Forensic Analysis of Linux Memory Snapshots},
year = {2022},
isbn = {9781450397049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545948.3545980},
doi = {10.1145/3545948.3545980},
abstract = {The development and research of tools for forensically analyzing Linux memory snapshots have stalled in recent years as they cannot deal with the high degree of configurability and fail to handle security advances like structure layout randomization. Existing tools such as Volatility and Rekall require a pre-generated profile of the operating system, which is not always available, and can be invalidated by the smallest source code or configuration changes in the kernel. In this paper, we create a reference model of the control and data flow of selected representative Linux kernels. Using this model, ABI properties, and Linux’s own runtime information, we apply a configuration- and instruction-set-agnostic structural matching between the reference model and the loaded kernel to obtain enough information to drive all practically relevant forensic analyses. We implemented our approach in Katana 1, and evaluated it against Volatility. Katana is superior where no perfect profile information is available. Furthermore, we show correct functionality on an extensive set of 85 kernels with different configurations and 45 realistic snapshots taken while executing popular Linux distributions or recent versions of Android from version 8.1 to 11. Our approach translates to other CPU architectures in the Internet-of-Things (IoT) device domain such as MIPS and ARM64 as we show by analyzing a TP-Link router and a smart camera. We also successfully generalize to modified Linux kernels such as Android.},
booktitle = {Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {214–231},
numpages = {18},
keywords = {binary analysis, automated profile generation, memory forensics},
location = {Limassol, Cyprus},
series = {RAID '22}
}

@inproceedings{10.1145/3167086,
author = {P\^{\i}rlea, George and Sergey, Ilya},
title = {Mechanising Blockchain Consensus},
year = {2018},
isbn = {9781450355865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167086},
doi = {10.1145/3167086},
abstract = {We present the first formalisation of a blockchain-based distributed consensus protocol with a proof of its consistency mechanised in an interactive proof assistant. Our development includes a reference mechanisation of the block forest data structure, necessary for implementing provably correct per-node protocol logic. We also define a model of a network, implementing the protocol in the form of a replicated state-transition system. The protocol's executions are modeled via a small-step operational semantics for asynchronous message passing, in which packages can be rearranged or duplicated. In this work, we focus on the notion of global system safety, proving a form of eventual consistency. To do so, we provide a library of theorems about a pure functional implementation of block forests, define an inductive system invariant, and show that, in a quiescent system state, it implies a global agreement on the state of per-node transaction ledgers. Our development is parametric with respect to implementations of several security primitives, such as hash-functions, a notion of a proof object, a Validator Acceptance Function, and a Fork Choice Rule. We precisely characterise the assumptions, made about these components for proving the global system consensus, and discuss their adequacy. All results described in this paper are formalised in Coq.},
booktitle = {Proceedings of the 7th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {78–90},
numpages = {13},
keywords = {protocol verification, consensus, blockchain, Coq},
location = {Los Angeles, CA, USA},
series = {CPP 2018}
}

@article{10.1145/1543753.1543754,
author = {Venkataramani, Guru and Doudalis, Ioannis and Solihin, Yan and Prvulovic, Milos},
title = {MemTracker: An Accelerator for Memory Debugging and Monitoring},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/1543753.1543754},
doi = {10.1145/1543753.1543754},
abstract = {Memory bugs are a broad class of bugs that is becoming increasingly common with increasing software complexity, and many of these bugs are also security vulnerabilities. Existing software and hardware approaches for finding and identifying memory bugs have a number of drawbacks including considerable performance overheads, target only a specific type of bug, implementation cost, and inefficient use of computational resources.This article describes MemTracker, a new hardware support mechanism that can be configured to perform different kinds of memory access monitoring tasks. MemTracker associates each word of data in memory with a few bits of state, and uses a programmable state transition table to react to different events that can affect this state. The number of state bits per word, the events to which MemTracker reacts, and the transition table are all fully programmable. MemTracker's rich set of states, events, and transitions can be used to implement different monitoring and debugging checkers with minimal performance overheads, even when frequent state updates are needed. To evaluate MemTracker, we map three different checkers onto it, as well as a checker that combines all three. For the most demanding (combined) checker with 8 bits state per memory word, we observe performance overheads of only around 3%, on average, and 14.5% worst-case across different benchmark suites. Such low overheads allow continuous (always-on) use of MemTracker-enabled checkers, even in production runs.},
journal = {ACM Trans. Archit. Code Optim.},
month = {jul},
articleno = {5},
numpages = {33},
keywords = {memory access monitoring, Accelerator, debugging}
}

@inproceedings{10.1145/2659651.2660516,
author = {Ramli, Ahmad Kamal and Djemame, Karim},
title = {Autonomic Management for Convergent Networks to Support Robustness of Appliance Technologies},
year = {2014},
isbn = {9781450330336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659651.2660516},
doi = {10.1145/2659651.2660516},
abstract = {Autonomic management within autonomic computing framework is considered as the future and viable solution for many appliances, either in software or hardware. Nevertheless, its current research application in computer networks is mainly visible in the intra domain space, and less attention is given to inter domain between one core network and another. This paper reviews some of the work on autonomic management and presents a framework that can be extended to a global and universal solution, such as fulfilling demand on bandwidth management, Quality of Service (QOS), and Service Level Agreements (SLA). The autonomic computing self- features are considered to show the viability of the proposed framework.},
booktitle = {Proceedings of the 7th International Conference on Security of Information and Networks},
pages = {47–51},
numpages = {5},
keywords = {Next generation Networks, Autonomous and Adaptive Security, Adaptive Architecture, Bandwidth Management, Autonomic Management, Service Level Agreements, Information Assurance},
location = {Glasgow, Scotland, UK},
series = {SIN '14}
}

@article{10.14778/3574245.3574278,
author = {Xu, Zihuan and Chen, Lei},
title = {L2chain: Towards High-Performance, Confidential and Secure Layer-2 Blockchain Solution for Decentralized Applications},
year = {2022},
issue_date = {December 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3574245.3574278},
doi = {10.14778/3574245.3574278},
abstract = {With the rapid development of blockchain, the concept of decentralized applications (DApps), built upon smart contracts, has attracted much attention in academia and industry. However, significant issues w.r.t. system throughput, transaction confidentiality, and the security guarantee of the DApp transaction execution and order correctness hinder the border adoption of blockchain DApps.To address these issues, we propose L2chain, a novel blockchain framework aiming to scale the system through a layer-2 network where DApps process transactions in the layer-2 network and only the system state digest, acting as the state integrity proof, is maintained on-chain. To achieve high performance, we introduce the split-execute-merge (SEM) transaction processing workflow with the help of the RSA accumulator, allowing DApps to lock and update a part of the state digest in parallel. We also design a witness cache mechanism for DApp executors to reduce the transaction processing latency. To fulfill confidentiality, we leverage the trusted execution environment (TEE) for DApps to execute encrypted transactions off-chain. To ensure transaction execution and order correctness, we propose a two-step execution process for DApps to prevent attacks (i.e., rollback attacks) from subverting the state transition. Extensive experiments have demonstrated that L2chain can achieve 1.5X to 42.2X and 7.1X to 8.9X throughput improvements in permissioned and permissionless settings respectively.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {986–999},
numpages = {14}
}

@inproceedings{10.1145/1460877.1460902,
author = {Barry, Bazara I. A. and Chan, H. Anthony},
title = {On the Performance of a Hybrid Intrusion Detection Architecture for Voice over IP Systems},
year = {2008},
isbn = {9781605582412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1460877.1460902},
doi = {10.1145/1460877.1460902},
abstract = {Voice over IP (VoIP) environments pose challenging threats to Intrusion Detection Systems (IDSs). Services over VoIP systems are provided by multiple interacting protocols, each with its own vulnerabilities. This scheme could result in novel and more complex attacks, and requires cross-protocol aware IDSs. Furthermore, VoIP devices may suffer a full or partial service loss if the syntax or semantics of the aforementioned protocols are violated. Usually, a single detection approach is suited to identify a subset of the security violations to which a system is subject in VoIP environments. Therefore, a hybrid approach that combines the strengths and avoids the weaknesses of various approaches is needed. In this paper, we discuss the performance and the detection accuracy of a hybrid, host-based intrusion detection system suitable for VoIP environments. Our system has two combined detection modules, namely, a specification-based and a signature-based module. Both modules use State Machines and State Transition Analysis Techniques to model proper protocols' behaviors and potential attacks. Both modules address the issues related to syntax and semantics anomaly detection for the monitored protocols. In addition, our architecture provides a cross-protocol framework for various protocols to exchange useful detection information in real time. We implement our proposed architecture in a network simulator, alongside implementing a variety of attacks to test the credibility of the design. The implemented IDS shows an excellent detection accuracy, and low runtime impact on the performance of the VoIP system.},
booktitle = {Proceedings of the 4th International Conference on Security and Privacy in Communication Netowrks},
articleno = {19},
numpages = {10},
keywords = {hybrid detection, intrusion detection, VoIP, performance},
location = {Istanbul, Turkey},
series = {SecureComm '08}
}

@inproceedings{10.1145/2517881.2517891,
author = {Jin, Xin and Krishnan, Ram and Sandhu, Ravi},
title = {Reachability Analysis for Role-Based Administration of Attributes},
year = {2013},
isbn = {9781450324939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517881.2517891},
doi = {10.1145/2517881.2517891},
abstract = {Attribute-based access control (ABAC) is well-known and increasingly prevalent. Nonetheless, administration of attributes is not well-studied so far. Recently, the Generalized User-Role Assignment model (GURA) was proposed to provide ARBAC97-style (administrative role-based access control) administration of user attributes. An attribute is simply a name-value pair, examples of which include clearance, group and affiliations. In GURA, user attributes are collectively administered by different administrative roles to enable distributed administration. Given an administrative policy that specifies the conditions under which administrative roles can modify user attributes, it is useful to understand whether an attribute of a particular user can reach a specific value because user attributes are used for security-sensitive activities such as authentication, authorization and audit. In this paper, we study the user-attribute reachability problems in a restricted GURA model called rGURA. We formalize rGURA as a state transition system and show that the reachability problems for its general cases are PSPACE-complete. However, we do find polynomial-time solutions to reachability problems for limited versions of rGURA that are still useful in practice. The algorithms not only answer reachability problem but also provide a plan of sequential attribute updates by one or more administrators in order to reach particular values for user attributes. rGURA is relatively simple and practical. It is likely that other proposals will subsume the functionality of rGURA and thereby subsume its complexity results.},
booktitle = {Proceedings of the 2013 ACM Workshop on Digital Identity Management},
pages = {73–84},
numpages = {12},
keywords = {attributes, administration, reachability analysis},
location = {Berlin, Germany},
series = {DIM '13}
}

@article{10.1145/1108906.1108908,
author = {Zhang, Xinwen and Parisi-Presicce, Francesco and Sandhu, Ravi and Park, Jaehong},
title = {Formal Model and Policy Specification of Usage Control},
year = {2005},
issue_date = {November 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1094-9224},
url = {https://doi.org/10.1145/1108906.1108908},
doi = {10.1145/1108906.1108908},
abstract = {The recent usage control model (UCON) is a foundation for next-generation access control models with distinguishing properties of decision continuity and attribute mutability. A usage control decision is determined by combining authorizations, obligations, and conditions, presented as UCONABC core models by Park and Sandhu. Based on these core aspects, we develop a formal model and logical specification of UCON with an extension of Lamport's temporal logic of actions (TLA). The building blocks of this model include: (1) a set of sequences of system states based on the attributes of subjects, objects, and the system, (2) authorization predicates based on subject and object attributes, (3) usage control actions to update attributes and accessing status of a usage process, (4) obligation actions, and (5) condition predicates based on system attributes. A usage control policy is defined as a set of temporal logic formulas that are satisfied as the system state changes. A fixed set of scheme rules is defined to specify general UCON policies with the properties of soundness and completeness. We show the flexibility and expressive capability of this formal model by specifying the core models of UCON and some applications.},
journal = {ACM Trans. Inf. Syst. Secur.},
month = {nov},
pages = {351–387},
numpages = {37},
keywords = {security policy, Access control, formal specification, usage control}
}

@inproceedings{10.1145/3468264.3468546,
author = {Zhang, Wuqi and Wei, Lili and Li, Shuqing and Liu, Yepang and Cheung, Shing-Chi},
title = {\DH{}Archer: Detecting on-Chain-off-Chain Synchronization Bugs in Decentralized Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468546},
doi = {10.1145/3468264.3468546},
abstract = {Since the emergence of Ethereum, blockchain-based decentralized applications (DApps) have become increasingly popular and important. To balance the security, performance, and costs, a DApp typically consists of two layers: an on-chain layer to execute transactions and store crucial data on the blockchain and an off-chain layer to interact with users. A DApp needs to synchronize its off-chain layer with the on-chain layer proactively. Otherwise, the inconsistent data in the off-chain layer could mislead users and cause undesirable consequences, e.g., loss of transaction fees. However, transactions sent to the blockchain are not guaranteed to be executed and could even be reversed after execution due to chain reorganization. Such non-determinism in the transaction execution is unique to blockchain. DApp developers may fail to perform the on-chain-off-chain synchronization accurately due to their lack of familiarity with the complex transaction lifecycle. In this work, we investigate the challenges of synchronizing on-chain and off-chain data in Ethereum-based DApps. We present two types of bugs that could result in inconsistencies between the on-chain and off-chain layers. To help detect such on-chain-off-chain synchronization bugs, we introduce a state transition model to guide the testing of DApps and propose two effective oracles to facilitate the automatic identification of bugs. We build the first testing framework, \DH{}Archer, to detect on-chain-off-chain synchronization bugs in DApps. We have evaluated \DH{}Archer on 11 popular real-world DApps. \DH{}Archer achieves high precision (99.3%), recall (87.6%), and accuracy (89.4%) in bug detection and significantly outperforms the baseline methods. It has found 15 real bugs in the 11 DApps. So far, six of the 15 bugs have been confirmed by the developers, and three have been fixed. These promising results demonstrate the usefulness of \DH{}Archer.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {553–565},
numpages = {13},
keywords = {Software testing, Blockchain, Decentralized applications, DApps},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/1882486.1882496,
author = {Bando, Masanori and Artan, N. Sertac and Chao, H. Jonathan},
title = {LaFA: Lookahead Finite Automata for Scalable Regular Expression Detection},
year = {2009},
isbn = {9781605586304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882486.1882496},
doi = {10.1145/1882486.1882496},
abstract = {Although Regular Expressions (RegExes) have been widely used in network security applications, their inherent complexity often limits the total number of RegExes that can be detected using a single chip for a reasonable throughput. This limit on the number of RegExes impairs the scalability of today's RegEx detection systems. The scalability of existing schemes is generally limited by the traditional per character state processing and state transition detection paradigm. The main focus of existing schemes is in optimizing the number of states and the required transitions, but not the suboptimal character-based detection method. Furthermore, the potential benefits of reduced number of operations and states using out-of-sequence detection methods have not been explored. In this paper, we propose Looka-head Finite Automata (LaFA) to perform scalable RegEx detection using very small amount of memory. LaFA's memory requirement is very small due to the following three areas of effort described in this paper: (1) Different parts of a RegEx, namely RegEx components, are detected using different detectors, each of which is specialized and optimized for the detection of a certain RegEx component. (2) We systematically reorder the RegEx component detection sequence, which provides us with new possibilities for memory optimization. (3) Many redundant states in classical finite automata are identified and eliminated in LaFA. Our simulations show that LaFA requires an order of magnitude less memory compared to today's state-of-the-art RegEx detection systems. A single commodity Field Programmable Gate Array (FPGA) chip can accommodate up to twenty-five thousand (25k) RegExes. Based on the throughput of our LaFA prototype on FPGA, we estimated that a 34-Gbps throughput can be achieved.},
booktitle = {Proceedings of the 5th ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {40–49},
numpages = {10},
keywords = {finite automation, deep packet inspection, regular expressions, network intrusion detection system, FPGA, LaFA},
location = {Princeton, New Jersey},
series = {ANCS '09}
}

@inproceedings{10.5555/2527218.2527220,
author = {Benard, Vincent and Richard, Philippe and Vanderhaegen, Fr\'{e}d\'{e}ric and Caulier, Patrice},
title = {Contribution to the Characterization and Identification of Human Stability with Regard to Safety: Application to Guided Transport Systems},
year = {2012},
isbn = {9781921770159},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {This paper presents an original contribution based on the concept of human stability by identifying the associated risks as part of the safety system assessment. The difficulties to take into account human factors in safety studies are first highlighted and definitions of new ways for the integration of human factors based on the existing concepts of stability and resilience are proposed. Although the stability concept is usually defined around a sustainable equilibrium point that induces a feeling of safety control during normal operation, it appears that the stable behaviour of a human operator can lead to risk in certain situations or contexts such as hypo-vigilance, inattention and so on. The core of this paper lays the foundation of human stability for risks assessment. Here, Human stability is defined as the ability of the operator to stay in a stable operating state under specified conditions. This concept is formalized and 3 modes of stability are developed (time, frequency and sequential modes) in order to identify states and change of states of the human stability. The concept of human stability is then applied in the framework of ERTMS/ETCS and shows that sequences of Human stability states and changes of Human stability states may be precursors of risk. Finally, some perspectives highlight the interest of human stability for the definition of risk indicators to assess system safety, by considering the Human operator as a safety/security multi-criteria sensor for the supervision of human-machine systems.},
booktitle = {Proceedings of the Australian System Safety Conference - Volume 145},
pages = {13–20},
numpages = {8},
keywords = {safety, transportation application, resilience, human stability},
location = {Brisbane, Australia},
series = {ASSC '12}
}

@inproceedings{10.1145/2676723.2691922,
author = {Ortiz, Ariel},
title = {A Bottom-Up Approach to Teaching Server-Side Web Development Skills (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2691922},
doi = {10.1145/2676723.2691922},
abstract = {When dealing with the topic of back-end programming many CS web development courses typically focus on how to use a popular web framework, for example Spring MVC or Ruby on Rails. The problem with this approach is that students will most likely end up using some other different framework or technology if ever they decide to become professional web developers. Our students need to learn concepts and skills that serve as a foundation to learn whatever different technologies are used now or happen to appear in the future. This poster presents the author's experience on using a bottom-up approach to teach the fundamental aspects of how the HTTP protocol works, and how this knowledge can be used to get a deep understanding of the inner workings of the web by building a simple yet complete server-side web framework. Using Node.js as the development platform, students are able to take TCP sockets as the building blocks for higher-level web abstractions. This approach allows covering a variety of specific topics that are essential for a professional web developer: request and response structure and headers, HTTP methods, form processing, cookies and sessions, text encodings, MVC software architectural pattern, database integration using ORM (Object-Relational Mapping), REST (Representational State Transfer) architecture, security issues (HTTPS protocol, common web vulnerabilities), and client-side integration using AJAX (Asynchronous JavaScript and XML). Anecdotal evidence shows that students with this knowledge repertoire are better suited for learning, using and debugging new and existing web technologies.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {678},
numpages = {1},
keywords = {web development, server-side programming, node.js, javascript},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@article{10.1145/3495534,
author = {Kang, Liuwang and Shen, Haiying},
title = {Detection and Mitigation of Sensor and CAN Bus Attacks in Vehicle Anti-Lock Braking Systems},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3495534},
doi = {10.1145/3495534},
abstract = {For a modern vehicle, if the sensor in a vehicle anti-lock braking system (ABS) or controller area network (CAN) bus is attacked during a brake process, the vehicle will lose driving direction control and the driver’s life will be highly threatened. However, current methods for detecting attacks are not sufficiently accurate, and no method can provide attack mitigation. To ensure vehicle ABS security, we propose an attack detection method to accurately detect both sensor attack (SA) and CAN bus attack in a vehicle ABS, and an attack mitigation strategy to mitigate their negative effects on the vehicle ABS. In our attack detection method, we build a vehicle state space equation that considers the real-time road friction coefficient to predict vehicle states (i.e., wheel speed and longitudinal brake force) with their previous values. Based on sets of historical measured vehicle states, we develop a search algorithm to find out attack changes (vehicle state changes because of attack) by minimizing errors between the predicted vehicle states and the measured vehicle states. In our attack mitigation strategy, attack changes are subtracted from the measured vehicle states to generate correct vehicle states for a vehicle ABS. We conducted the first real SA experiments to show how a magnet affects sensor readings. Our simulation results demonstrate that our attack detection method can detect SA and CAN bus attack more accurately compared with existing methods, and also that our attack mitigation strategy almost eliminates the attack’s effects on a vehicle ABS.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {jan},
articleno = {9},
numpages = {24},
keywords = {attack mitigation, Vehicle ABS, attack detection, CAN bus attack, sensor attack}
}

@proceedings{10.1145/2491260,
title = {ANC '13: Proceedings of the Second ACM MobiHoc Workshop on Airborne Networks and Communications},
year = {2013},
isbn = {9781450322089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizers, we extend a warm welcome to the second ACM MobiHoc workshop on "Airborne Networks and Communications" to all participants. An airborne network is a mobile network consisting of manned and unmanned air vehicles as well as ground vehicles. The ability of ground and air vehicles to communicate voice, video, and data offers enhanced safety and efficiency for the next generation (NextGen) air transportation systems. Airborne networks can benefit many civilian applications such as air-traffic control, border patrol, and search and rescue missions.This workshop is a result of the ideas that emerged from the meetings held over the past few years on topics that focused on cyber-physical systems (CPS) for air transportation as well as NextGen aviation systems. We believe that this workshop is an opportunity for researchers engaged in airborne networking and communications to discuss state-of-the-art, share their research results with their peers, and develop directions for future research in this emerging field.Airborne networking is a cyber-physical system. While computation, communication and networking elements form the cyber components of the system, flight-paths, maneuver geometries, and multi-mode resources including ground-based nodes and control stations form the physical components of the CPS. The synergy between the cyber and physical components, if explored and exploited, will significantly enhance the safety and security capabilities of Next Generation air transportation systems. However, fundamental design principles which are needed to explore this synergy do not exist and experimental datasets which are needed to develop such design principles are beyond the reach of academic community.During this workshop, we will hear from experienced speakers coming from the industry, universities, and federal laboratories on topics covering theoretical foundations and models for mobility, connectivity, and coverage, cyber-physical system perspective of airborne networks, airborne/satellite communication and networking platforms and strategies, protocols for secure information sharing, swarming, collaboration, and self-organization, network trials, test-beds, experiments, and measurements and applications of airborne networking to real world domains such as border patrol, air-traffic control, search and rescue missions, and unmanned cargo. We look forward to your active participation in this workshop.},
location = {Bangalore, India}
}

@inproceedings{10.1145/3307650.3322216,
author = {Sakalis, Christos and Kaxiras, Stefanos and Ros, Alberto and Jimborean, Alexandra and Sj\"{a}lander, Magnus},
title = {Efficient Invisible Speculative Execution through Selective Delay and Value Prediction},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322216},
doi = {10.1145/3307650.3322216},
abstract = {Speculative execution, the base on which modern high-performance general-purpose CPUs are built on, has recently been shown to enable a slew of security attacks. All these attacks are centered around a common set of behaviors: During speculative execution, the architectural state of the system is kept unmodified, until the speculation can be verified. In the event that a misspeculation occurs, then anything that can affect the architectural state is reverted (squashed) and re-executed correctly. However, the same is not true for the microarchitectural state. Normally invisible to the user, changes to the microarchitectural state can be observed through various side-channels, with timing differences caused by the memory hierarchy being one of the most common and easy to exploit. The speculative side-channels can then be exploited to perform attacks that can bypass software and hardware checks in order to leak information. These attacks, out of which the most infamous are perhaps Spectre and Meltdown, have led to a frantic search for solutions.In this work, we present our own solution for reducing the microarchitectural state-changes caused by speculative execution in the memory hierarchy. It is based on the observation that if we only allow accesses that hit in the L1 data cache to proceed, then we can easily hide any microarchitectural changes until after the speculation has been verified. At the same time, we propose to prevent stalls by value predicting the loads that miss in the L1. Value prediction, though speculative, constitutes an invisible form of speculation, not seen outside the core. We evaluate our solution and show that we can prevent observable microarchitectural changes in the memory hierarchy while keeping the performance and energy costs at 11% and 7%, respectively. In comparison, the current state of the art solution, InvisiSpec, incurs a 46% performance loss and a 51% energy increase.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {723–735},
numpages = {13},
keywords = {side-channel attacks, speculative execution, caches},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@proceedings{10.1145/2248326,
title = {Airborne '12: Proceedings of the First ACM MobiHoc Workshop on Airborne Networks and Communications},
year = {2012},
isbn = {9781450312905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizers, we extend a warm welcome to the first ACM MobiHoc workshop on "Airborne Networks and Communications" to all participants. An airborne network is a mobile network consisting of manned and unmanned air vehicles as well as ground vehicles. The ability of ground and air vehicles to communicate voice, video, and data offers enhanced safety and efficiency for the next generation (NextGen) air transportation systems. Airborne networks can benefit many civilian applications such as air-traffic control, border patrol, and search and rescue missions.This workshop is a result of the ideas that emerged from the meetings held over the past few years on topics that focused on cyber-physical systems (CPS) for air transportation as well as NextGen aviation systems. We believe that the time is right for airborne networking and communications to be part of main stream conferences. We believe that this workshop is an opportunity for researchers engaged in airborne networking and communications to discuss state-of-the-art, share their research results with their peers, and develop directions for future research in this emerging field.Airborne networking is a cyber-physical system. While computation, communication and networking elements form the cyber components of the system, flight-paths, maneuver geometries, and multi-mode resources including ground-based nodes and control stations form the physical components of the CPS. The synergy between the cyber and physical components, if explored and exploited, will significantly enhance the safety and security capabilities of Next Generation air transportation systems. However, fundamental design principles which are needed to explore this synergy do not exist and experimental datasets which are needed to develop such design principles are beyond the reach of academic community.During this workshop, we will hear from experienced speakers coming from the industry, universities, and federal laboratories on topics covering theoretical foundations and models for mobility, connectivity, and coverage, cyber-physical system perspective of airborne networks, airborne/satellite communication and networking platforms and strategies, protocols for secure information sharing, swarming, collaboration, and self-organization, network trials, test-beds, experiments, and measurements and applications of airborne networking to real world domains such as border patrol, air-traffic control, search and rescue missions, and unmanned cargo. We look forward to your active participation in this workshop.},
location = {Hilton Head, South Carolina, USA}
}

@inproceedings{10.1145/3357223.3365759,
author = {Xu, Charles and Ilyevskiy, Dmitry},
title = {Isopod: An Expressive DSL for Kubernetes Configuration},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3365759},
doi = {10.1145/3357223.3365759},
abstract = {Kubernetes is an open-source cluster orchestration system for containerized workloads to reduce idiosyncrasy across cloud vendors [2]. Using Kubernetes, Cruise has built a multi-tenant platform with thousands of cores and tens of terabytes of memory. Such a scale is possible in part thanks to the declarative abstraction of Kubernetes, where desired states are described in YAML manifests [5].However, YAML as a data serialization format is unfit for workload specification. Structured data in YAML are untyped and prone to wrong indents and missing fields. Due to poor meta-programming support, composing YAML with control logic---loops and branches---suffers from YAML fragmentation and indentation tracking (example at bit.ly/yml-hell). Moreover, YAML manifests are often generated by filling a shared template with cluster-specific parameters---the image tag and the replica count might differ in development and production environments. Existing templating tools---Helm [11], Kustomize [9], Kapitan [7] and the likes---assume these parameters are statically known and use CLIs to query dynamic ones, such as secrets stored in HashiCorp Vault [10]. Such scheme is hard to test, since side effects escape through CLIs, and highly depends on the execution environment, since CLI versions vary across machines or might not exist. Not least, YAML manifests describe the eventual state but not how existing workloads will be affected. Blindly applying the manifest---for example, from a stale version of code---can be disastrous and cause unexpected outages.Isopod presents an alternative configuration paradigm by treating Kubernetes objects as first-class citizens. Without intermediate YAML artifacts, Isopod renders Kubernetes objects directly in Protocol Buffers [8], so they are strongly typed and consumed directly by the Kubernetes API. With Isopod, configurations are scripted in Starlark [3], a Python dialect by Google also used by Bazel [1] and Buck [4] build systems. To replace CLI dependencies, Isopod extends Starlark with runtime built-ins to access services and utilities such as Vault, Kubernetes apiserver, Base64 encoder, and UUID generator, etc. Isopod uses a separate runtime for unit tests to mock all built-ins, providing test coverage that was not possible before.Isopod is also hermetic and secure. The common reliance on the kubeconfig file for cluster authentication leaks secrets to disk, a security risk if working from a shared host, such as a cluster node or CICD worker. Instead, Isopod builds Oauth2 tokens [6] to the target cluster using the Identity &amp; Access Management (IAM) service of the cloud vendor. Application secrets are stored in Vault and queried at runtime. Hence, no secrets escape to the disk. In fact, Isopod prohibits disk IO except for loading Starlark modules from other scripts. No external libraries can be loaded unless explicitly implemented as an Isopod built-in. Distributed as a single binary, Isopod is self-contained with all dependencies.Finally, Isopod is extensible. Protobuf packages of Kubernetes API groups added in the future can be loaded in the same way. Because built-ins are modular and pluggable, users can easily implement and register new built-ins with the Isopod runtime to support any Kubernetes vendors. Isopod offers many other features, such as object life cycle management and parallel rollout to multiple clusters, which is impossible if using kubeconfig. In dry-run mode, Isopod displays intended actions from the current code change as a YAML diff against live objects in the cluster to avoid unexpected configuration change.Since the adoption of Isopod, the PaaS team at Cruise has migrated 14 applications and added another 16 without outage or regression, totaling around 10,000 lines of Starlark. The migration results in up to 60% reduction in code size and 80% faster rollout due to code reuse, cluster parallelism, and the removal of YAML intermediaries. All unit tests take less than 10 secs to finish. Isopod is open source at github.com/cruise-automation/isopod.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {483},
numpages = {1},
keywords = {Configuration Language, Cluster Orchestration},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.1145/67386.67426,
author = {Martin, Bruce},
title = {Concurrent Programming vs. Concurrency Control: Shared Events or Shared Data},
year = {1988},
isbn = {0897913043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67386.67426},
doi = {10.1145/67386.67426},
abstract = {Two views of concurrency in an object system exist. Those pursuing concurrent programming believe that activities in the real world are inherently concurrent and therefore objects are themselves active. Objects engage in shared events by sending and receiving messages. Communicating Sequential Processes [Hoar85a] and Actors [Agha86a] embrace this view. On the other hand, those pursuing models of concurrency control believe that objects are data and that concurrent access to data needs to be controlled by the system according to some correctness notion. Database transactions, atomic objects [Weih84a, Schw84a] and nested objects [Mart88a] embrace this view.Concurrent programming, in our view, places a significant burden on programming. Correct concurrent behavior is specified as combinations of interactions within a potentially large set of concurrent objects. A programmer must verify that the implementations of all the objects never produce undesirable interactions. Correctness of concurrent behavior is left to the programmer.We are pursuing models embracing concurrency control primarily because a programmer is not required to consider concurrency. The operations on an object can be specified in terms of preconditions and postconditions and traditional program verification techniques can be used to verify an operation's implementation. A programmer only considers the serial behavior of an object in isolation; he need not concern himself with how other concurrent activities might affect the object. Correctness of interleavings is left to the system.Serializability is the usual correctness notion for concurrency control algorithms. In transaction terminology, each competing transaction executes a sequence of basic actions. Any interleaving of the actions is correct if it is equivalent to some serial execution of the transaction. Serializability allows a transaction to be programmed in isolation, that is without considering possible interleavings with other transactions. The system may indeed interleave the actions of several transactions but it is up to the system to make the interleaving appear serial.Concurrent programming is apparently more general. A programmer can implement anything, including undesirable interactions like deadlock. The price for this generality is that the programmer must reason about global orderings of events and thus correctness is difficult to show.The traditional transaction model is not general enough for programming shared object systems. For example, several researchers, [Bern87a, Garc87a, Pu88a], have recognized that transactions are too restrictive for long-lived activities. The problem is that the transaction model is too conservative. Only reading and writing a data item at a single layer of abstraction is modeled. Once a read-write, write-read or write-write dependency is established between two transactions, it remains for the life of the transaction and limits further interleavings.Our approach is to discover and explore less restrictive correctness notions that still allow programmers to implement operations on objects in isolation. In [Mart88a] we present two such correctness notions: externally serializable computations and semantically verifiable nonserializable computations. Both correctness notions assume the nested object model. In [Mart87a] we give a nested object solution to the Dining Philosophers' Problem [Dijk71a]. Nested objects incorporate both the semantics of an object and the data abstraction hierarchy of an object.Nested objects form a nested object system. A nested object system is hierarchical; objects exist at different levels of the system. The execution of an operation on an object at level i results in the execution of operations on objects at level i-1. However, only top level objects are viewed externally.A computation at level i is a description of the state change made to level i objects and the return values produced by executing a partially ordered set of operations on level i objects. The computations at each level together form an n-level system computation.Externally serializable computations are n-level system computations in which the top level objects are left in states that could be produced by serial computations. However, lower level objects may be left in states that no serial computation could produce. Because both data abstraction hierarchies and operations semantics are considered in the nested object model, dependencies established between concurrent computations can be systematically ignored. Long-lived computations can execute efficiently if dependencies can later be ignored.Nested objects are more general than other models of concurrency control. Transactions are two-level nested objects that read and write basic data items. Atomic objects are two-level nested objects that perform abstract operations.The 1988 Object Based Concurrent Programming Workshop did not directly address the differences between concurrent programming and concurrency control. Perhaps future workshops can contrast the generality, the applicability, the programmability, the security and the performance implications of models from both concurrent programming and concurrency control.},
booktitle = {Proceedings of the 1988 ACM SIGPLAN Workshop on Object-Based Concurrent Programming},
pages = {142–144},
numpages = {3},
location = {San Diego, California, USA},
series = {OOPSLA/ECOOP '88}
}

@article{10.1145/67387.67426,
author = {Martin, Bruce},
title = {Concurrent Programming vs. Concurrency Control: Shared Events or Shared Data},
year = {1988},
issue_date = {April 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/67387.67426},
doi = {10.1145/67387.67426},
abstract = {Two views of concurrency in an object system exist. Those pursuing concurrent programming believe that activities in the real world are inherently concurrent and therefore objects are themselves active. Objects engage in shared events by sending and receiving messages. Communicating Sequential Processes [Hoar85a] and Actors [Agha86a] embrace this view. On the other hand, those pursuing models of concurrency control believe that objects are data and that concurrent access to data needs to be controlled by the system according to some correctness notion. Database transactions, atomic objects [Weih84a, Schw84a] and nested objects [Mart88a] embrace this view.Concurrent programming, in our view, places a significant burden on programming. Correct concurrent behavior is specified as combinations of interactions within a potentially large set of concurrent objects. A programmer must verify that the implementations of all the objects never produce undesirable interactions. Correctness of concurrent behavior is left to the programmer.We are pursuing models embracing concurrency control primarily because a programmer is not required to consider concurrency. The operations on an object can be specified in terms of preconditions and postconditions and traditional program verification techniques can be used to verify an operation's implementation. A programmer only considers the serial behavior of an object in isolation; he need not concern himself with how other concurrent activities might affect the object. Correctness of interleavings is left to the system.Serializability is the usual correctness notion for concurrency control algorithms. In transaction terminology, each competing transaction executes a sequence of basic actions. Any interleaving of the actions is correct if it is equivalent to some serial execution of the transaction. Serializability allows a transaction to be programmed in isolation, that is without considering possible interleavings with other transactions. The system may indeed interleave the actions of several transactions but it is up to the system to make the interleaving appear serial.Concurrent programming is apparently more general. A programmer can implement anything, including undesirable interactions like deadlock. The price for this generality is that the programmer must reason about global orderings of events and thus correctness is difficult to show.The traditional transaction model is not general enough for programming shared object systems. For example, several researchers, [Bern87a, Garc87a, Pu88a], have recognized that transactions are too restrictive for long-lived activities. The problem is that the transaction model is too conservative. Only reading and writing a data item at a single layer of abstraction is modeled. Once a read-write, write-read or write-write dependency is established between two transactions, it remains for the life of the transaction and limits further interleavings.Our approach is to discover and explore less restrictive correctness notions that still allow programmers to implement operations on objects in isolation. In [Mart88a] we present two such correctness notions: externally serializable computations and semantically verifiable nonserializable computations. Both correctness notions assume the nested object model. In [Mart87a] we give a nested object solution to the Dining Philosophers' Problem [Dijk71a]. Nested objects incorporate both the semantics of an object and the data abstraction hierarchy of an object.Nested objects form a nested object system. A nested object system is hierarchical; objects exist at different levels of the system. The execution of an operation on an object at level i results in the execution of operations on objects at level i-1. However, only top level objects are viewed externally.A computation at level i is a description of the state change made to level i objects and the return values produced by executing a partially ordered set of operations on level i objects. The computations at each level together form an n-level system computation.Externally serializable computations are n-level system computations in which the top level objects are left in states that could be produced by serial computations. However, lower level objects may be left in states that no serial computation could produce. Because both data abstraction hierarchies and operations semantics are considered in the nested object model, dependencies established between concurrent computations can be systematically ignored. Long-lived computations can execute efficiently if dependencies can later be ignored.Nested objects are more general than other models of concurrency control. Transactions are two-level nested objects that read and write basic data items. Atomic objects are two-level nested objects that perform abstract operations.The 1988 Object Based Concurrent Programming Workshop did not directly address the differences between concurrent programming and concurrency control. Perhaps future workshops can contrast the generality, the applicability, the programmability, the security and the performance implications of models from both concurrent programming and concurrency control.},
journal = {SIGPLAN Not.},
month = {sep},
pages = {142–144},
numpages = {3}
}

@article{10.1145/1037107.1037111,
author = {Campbell, Andrew T. and Schwartz, Mischa},
title = {ACM SIGCOMM Computer Communication Review},
year = {2001},
issue_date = {October 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/1037107.1037111},
doi = {10.1145/1037107.1037111},
abstract = {At some point in the future, how far out we do not exactly know, wireless access to the Internet will outstrip all other forms of access bringing the freedom of mobility to the way we access the web, communicate with each other, and conduct business. In short, the Internet is going mobile and wireless, perhaps quite soon.A number of diverse technologies are leading the charge, including, 3G cellular networks based on CDMA technology, a wide variety of what is deemed 2.5G cellular technologies (e.g., EDGE, GPRS and HDR), and IEEE 802.11 wireless local area networks (WLANs). Wireless ISPs will offer a number of these technologies to mobile users. In some case, handsets will come with software radios that simultaneously support multiple access technologies on-the-fly; for example, IEEE 802.11 for high-bandwidth access in urban areas and GPRS for wide area access in rural areas.Each technology has its pros and cons. First and second generation cellular systems offer wide area low bandwidth voice services based on analog and digital technology, respectively. The 3G cellular systems are designed to carry voice, video and data simultaneously, and offer data rates of 144 Kbps for fast-moving mobile users in vehicles, 384 Kbps for slower moving pedestrian users, and 2 Mbps from fixed locations. Note that all users within a cell share these data rates. The 3G networks offer higher capacity and increased spectral efficiency but retain a circuit-switched, hierarchical architecture. In contrast, WLAN offers even higher bandwidth and is considered IP friendly because it offers a link layer that is very similar to wired Ethernet. However, in comparison to 3G networks, WLAN only operates within the local area, only supports best effort services, and uses shared unlicensed spectrum where few quality assurances can be provided to users.Recently, there has been a considerable amount of press on the slow rollout of 3G. However, there are some signs for optimism. Japan's NTT DoCoMo started offering 3G services in October 2001 in the Tokyo area. This came after the initial postponement of the rollout of 3G services by providers in Japan and Europe. Since May 2001, 5,000 residents in the Tokyo area have been using new 3G phones that offer improved i-mode service and real-time videoconferencing. The initial video offering uses a 64 Kbps circuit that carries video and audio combined. One of the guest editors had the opportunity to use a trial handset to set up a video call to a colleague in a taxi while traveling through Tokyo. The real-time video call, which used MPEG4 technology, presented mixed service quality but the experience of setting up the call between two taxis was exciting. I-mode currently has 29 million subscribers in Japan and DoCoMo hopes to keep that figure rising with the new service offerings. The DoCoMo radio access network is based on WCDMA and the core network on ATM switching.Many carriers in the US and Europe will be keenly watching what is happening in Tokyo. Wireless providers in the United States are eager to follow suit but are rolling out service in phases with emphasis on 2.5G technologies such as GPRS, which provides an always-on connection to the Internet that allows users to toggle between surfing the web, a phone call, or text messaging without losing the connection. Carriers in Europe, which have invested more than $100 billion to buy 3G radio spectrum licenses and will need to invest another $100 billion for the build-out of the 3G networks, will be keeping a close watch on DoCoMo's successes and failures.The vast majority of WLAN deployed today is based on IEEE 802.11b operating at 2.4 Ghz and offering data rates up to 11 Mbps. Recently, a number of companies have demonstrated IEEE 802.11 a, which operates in the 5Ghz band and offers data rates up to 54 Mbps. In fact, Atheros Communications supports a "turbo-networking" mode that delivers 108 Mbps, roughly equivalent to Fast Ethernet. The cost of the 3G spectrum and the build-out of the 3G networks have been so prohibitive that many operators have been pushed to the brink of bankruptcy. As a result, many small operators in Europe are sharing the cost of the build-out by sharing core and radio access network infrastructure. In contrast, WLAN infrastructure operates in unlicensed frequency bands and is very cheap in comparison to cellular equipment. Cheap, because WLAN base-station transceivers are priced at less than $1,000, and transceiver cards are around $100 or come built into computers. Public wireless LANs can handle large volumes of data at significantly lower costs compared to leading 3G technologies. The cost benefit and bandwidth differential offered by WLAN technology makes it a disruptive technology as the cellular operators migrate from 2G to 3G.Disruptive technologies are characterized as being cheaper and of lower performance than sustainin g technologies (e.g., 2.5G or 3G solutions). Most public wireless networks and enterprise networks use WLAN, not because it is more secure, robust or spectrally efficient, but simply because it is cheap, offers high bandwidth, makes networks easy to build and configure, and, importantly, it works. Typically, customers are not initially satisfied with the performance offered by disruptive technologies when they are first introduced. For WLAN to compete in the marketplace with 2.5G and 3G solutions, public WLAN operators would need to be capable of building metropolitan area networks that provided suitable support for voice-over-IP there by enabling voice communications. Sharing unlicensed spectrum means that wireless ISPs cannot build managed networks where services are tightly controlled, in isolation from other operators, as a means of assuring performance. Historically, however, disruptive technologies have tended to resolve such performance problems as they mature and begin to capture market share.Examples of wireless extensions to Internet are all around us today. Here in New York City many companies, university campuses, coffee shops and stores offer wireless access to the web using WLAN technology. Columbia University, for example, provides students and faculty wireless access to the web as they move around campus. Companies such as MobiStar and Waypoint provide wireless connections at hotels, airports and cafes. Around Manhattan, Starbucks coffee shops offer wireless access to the Internet. At the grassroots level, community groups are putting up wireless antennas around the New York City area and in other cities offering free access to Internet. Some predict that these "freenets", which have a feel reminiscent to Napster, will ultimately succumb to a sustained corporate challenge or new wireless ISPs that offer cheap services across dense urban areas. The road to success for such fledgling operators may be littered with a number of business, regulatory and performance obstacles.There are a number of companies, standards bodies, and industrial fora vying to define future wireless extensions to the Internet. The end result is that operators are faced with a large and confusing array of choices on how best to build next generation mobile networks. 3G systems offer support for seamless mobility, paging, and service quality but are built on complex and costly connection-oriented networking infrastructure that lacks the inherent flexibility, scalability, and cost effectiveness found in IP networks. In contrast, Mobile IP represents a simple and scalable global mobility solution but lacks support for fast handoff control, real-time location tracking, and authentication and distributed policy management found in cellular networks today. There has also been considerable interest in new emerging wireless technologies such as personal area networks, mobile ad hoc networks and sensor networks. How these technologies interwork with the global Internet is an active area of research.A number of micro-mobility protocols (e.g., Cellular IP, Hawaii, Hierarchical Mobile IP) and fast handoff schemes have been discussed in the IETF Mobile IP Working Group that address some of these performance and scalability issues. These protocols are designed for environments where mobile hosts change their point of attachment to the network so frequently that the basic Mobile IP protocol tunneling mechanism introduces network overhead in terms of increased delay, packet loss and signaling. For example, many real-time wireless applications (e.g., voice-over-IP) would experience noticeable degradation of service with frequent handoff. Establishment of new tunnels can introduce additional delays in the handoff process, causing packet loss and delayed delivery of data to applications. This delay is inherent in the round-trip incurred by Mobile IP as the registration request is sent to the home agent and the response sent back to the foreign agent. Micromobility protocols aim to handle local movement (e.g., within a domain) of mobile hosts without interaction with the Mobile IP enabled Internet. This has the benefit of reducing delay and packet loss during handoff and eliminating registration between mobile hosts and possibly distant home agents when mobile hosts remain inside their local coverage areas. Eliminating registration in this manner reduces the signaling load experienced by the network in support of mobility.As the numbers of wireless users grow so will the signaling overhead associated with mobility management. In cellular networks registration and paging techniques are used to minimize the signaling overhead and optimize mobility management performance. Currently, Mobile IP supports registration but not paging. An important characteristic of micro-mobility protocols is their ability to reduce the signaling overhead related to frequent mobile migrations taking into account a mobile host's operational mode (i.e., active or idle). When wireless access to Internet becomes the norm then Mobile IP will have to provide efficient and scalable location tracking in support of idle users, and IP paging in support of active communications. Support for "passive connectivity" to the wireless Internet balances a number of important design considerations. For example, only keeping the approximate location information of idle users requires significantly less signaling and thus reduces the load over the air interface and in the network. Reducing signaling over the air interfaces in this manner also has the benefit of preserving the power reserves of mobile hosts. Currently, the IETF Seamoby Working Group is tasked with developing an IP paging protocol.The papers in this special issue address a number of the issues and challenges discussed above. We received a total of 32 excellent submissions for this special issue -- a much greater response to our call for papers than we expected. The papers came from different regions around the world and addressed many different aspects of research. Each paper was reviewed by three or more experts, who evaluated the technical content and suitability of the paper for publication in this special issue. As guest editors of the special issue we had the very difficult job of selecting only six papers from those submitted. Several deserving papers could not be accommodated in this special issue because of space. We hope to see those papers appear later in ACM SIGCOMM Computer Communication Review.The first three papers in this special issue address a number of enhancements to Mobile IP and cellular networks to provide for better support for fast handoff and context transfer, wireless Internet telephony, and IP paging. The final three papers deal with the emerging technologies of personal area networks, mobile ad hoc networks and sensor networks.In the first paper, Jonathan Lennox, Kazutaka Murakami, Mehmet Karaul and Thomas F. La Porta, Lucent Technologies, discuss internetworking Internet telephony and wireless telecommunications networks. The authors propose a number of schemes to directly interconnect the 3G UMTS and SIP Internet telephony systems.The next paper by Rajeev Koodli and Charles E. Perkins, Nokia, deals with seamless handoff and context relocation in mobile networks. Context transfer refers to state information (e.g., QOS state) associated with a particular service (e.g., VoIP) that needs to be re-established with mobility. The authors show that fast handoff with context transfer at the network layer can support uninterrupted voice-over-IP services.The paper by Pars Mutaf and Claude Castelluccia, INRIA, proposes adaptive per-host IP paging. The authors observe that many of the existing IP paging proposals found in the literature promote the use of static or manually configured paging areas. The authors argue that there is a need for dynamic and adaptive paging area management that takes into account host mobility and traffic patterns in the network.Robin Kravets, Casey Carter and Luiz Magalhaes, University of Illinois, Urbana-Champaign, discuss cooperative approaches to user mobility. The authors propose the necessary networking functionality that allows groups of mobile devices (e.g., a set of devices that collectively comprise a personal area network) to interact and be seamlessly integrated into the Internet.In the next paper, Jyoti Raju and J. J. Garcia-Luna-Aceves UC Santa Cruz, present a new mobile ad hoc network routing protocol called source tracing and compare it with dynamic source routing (DSR). Both on-demand and table-driven implementations are considered.The final paper in this special issue, by Samir Goel and Tomasz Imielinski, Rutgers University, considers the problem of monitoring data in large sensor networks. The authors propose a prediction-based monitoring scheme that can be visualized by leveraging concepts and techniques found in image processing.There are many other technical challenges before Internet goes truly wireless and mobile. For example, there is a need to minimize the impact of mobility on TCP performance, resolve security issues over-the-air, and further study how best content can be pushed toward mobile users. Finally, in the wake of the recent attack in New York City, we anticipate new advances in rapidly deployable wireless infrastructure, self-configuring networks, and sensor networks - collectively forming disaster relief networks.As guest editors it has been a great pleasure to put together this issue. We would like to thank the authors for their contributions and the reviewers for their time, energy, and comments that helped shape this special issue. We hope you enjoy it as much as we do.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {oct},
pages = {20–24},
numpages = {5}
}

